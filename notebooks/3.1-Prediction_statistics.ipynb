{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions statistics\n",
    "\n",
    "Let's analyse the predictions made on our test datasplit.\n",
    "\n",
    "**Note** To run this notebook you previously have to run and save the predictions on a test dataset **with ground-truth labels**. See the Datasplit section in [3.0 notebook](./3.0-Computing_predictions.ipynb#Predicting-a-datasplit-txt-file) for reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from planktonclas import test_utils\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix,roc_auc_score\n",
    "from planktonclas.data_utils import load_image, load_class_names\n",
    "from planktonclas import paths, plot_utils\n",
    "from planktonclas.plot_utils import create_pred_path,plt_conf_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # To ignore UndefinedMetricWarning: [Recall/Precision/F-Score] is ill-defined and being set to 0.0 in labels with no [true/predicted] samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class names...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# User parameters to set\n",
    "TIMESTAMP = 'phytoplankton_vliz'           # timestamp of the model\n",
    "SPLIT_NAME = 'DS_split'                   # dataset split to predict\n",
    "MODEL_NAME = 'final_model.h5'         # model to use to make the mediction\n",
    "TOP_K = 5                             # number of top classes predictions to save\n",
    "\n",
    "# Set the timestamp\n",
    "paths.timestamp = TIMESTAMP\n",
    "\n",
    "# Load clas names\n",
    "class_names = load_class_names(splits_dir=paths.get_ts_splits_dir())\n",
    "\n",
    "# Load back the predictions\n",
    "pred_path = os.path.join(paths.get_predictions_dir(), '{}+{}+top{}.json'.format(MODEL_NAME, SPLIT_NAME, TOP_K))\n",
    "with open(pred_path) as f:\n",
    "    pred_dict = json.load(f)\n",
    "data = pd.DataFrame(pred_dict)\n",
    "data[['filenames', 'true_lab','pred_lab']].head(5)\n",
    "data['in_top5'] = data.apply(lambda row: row['true_lab'] in row['pred_lab'], axis=1)\n",
    "data['top1_correct'] = data.apply(lambda row: row['true_lab'] == row['pred_lab'][0], axis=1)\n",
    "data['true_label']=class_names[data['true_lab']]\n",
    "data['top1']=data.apply(lambda row: class_names[row['pred_lab'][0]], axis=1)\n",
    "data['probability']=data.apply(lambda row: row['pred_prob'][0], axis=1)\n",
    "flat_array=np.concatenate(np.array(pred_dict['pred_lab']))\n",
    "data['pred_lab'] = flat_array.reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the metrics\n",
    "\n",
    "Check [sklearn](https://scikit-learn.org/stable/modules/classes.html#classification-metrics) for more classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 accuracy: 84.3 %\n",
      "Top5 accuracy: 98.1 %\n",
      "\n",
      "\n",
      "Micro recall: 84.3 %\n",
      "Macro recall: 75.1 %\n",
      "Macro recall (no labels): 75.1 %\n",
      "Weighted recall: 84.3 %\n",
      "\n",
      "\n",
      "Micro precision: 84.3 %\n",
      "Macro precision: 81.7 %\n",
      "Macro precision (no labels): 81.7 %\n",
      "Weighted precision: 85.3 %\n",
      "\n",
      "\n",
      "Micro F1 score: 84.3 %\n",
      "Macro F1 score: 77.4 %\n",
      "Macro F1 score (no labels): 77.4 %\n",
      "Weighted F1 score: 84.4 %\n"
     ]
    }
   ],
   "source": [
    "true_lab, pred_lab = np.array(pred_dict['true_lab']), np.array(pred_dict['pred_lab'])\n",
    "\n",
    "top1 = test_utils.topK_accuracy(true_lab, pred_lab, K=1)\n",
    "top5 = test_utils.topK_accuracy(true_lab, pred_lab, K=5)\n",
    "\n",
    "print('Top1 accuracy: {:.1f} %'.format(top1 * 100))\n",
    "print('Top5 accuracy: {:.1f} %'.format(top5 * 100))\n",
    "\n",
    "labels = range(len(class_names))\n",
    "print('\\n')\n",
    "print('Micro recall: {:.1f} %'.format(100 * recall_score(true_lab, pred_lab[:, 0], labels=labels, average='micro')))\n",
    "print('Macro recall: {:.1f} %'.format(100 * recall_score(true_lab, pred_lab[:, 0], labels=labels, average='macro')))\n",
    "print('Macro recall (no labels): {:.1f} %'.format(100 * recall_score(true_lab, pred_lab[:, 0], average='macro')))\n",
    "print('Weighted recall: {:.1f} %'.format(100 * recall_score(true_lab, pred_lab[:, 0], labels=labels, average='weighted')))\n",
    "\n",
    "print('\\n')\n",
    "print('Micro precision: {:.1f} %'.format(100 * precision_score(true_lab, pred_lab[:, 0], labels=labels, average='micro')))\n",
    "print('Macro precision: {:.1f} %'.format(100 * precision_score(true_lab, pred_lab[:, 0], labels=labels, average='macro')))\n",
    "print('Macro precision (no labels): {:.1f} %'.format(100 * precision_score(true_lab, pred_lab[:, 0], average='macro')))\n",
    "print('Weighted precision: {:.1f} %'.format(100 * precision_score(true_lab, pred_lab[:, 0], labels=labels, average='weighted')))\n",
    "\n",
    "print('\\n')\n",
    "print('Micro F1 score: {:.1f} %'.format(100 * f1_score(true_lab, pred_lab[:, 0], labels=labels, average='micro')))\n",
    "print('Macro F1 score: {:.1f} %'.format(100 * f1_score(true_lab, pred_lab[:, 0], labels=labels, average='macro')))\n",
    "print('Macro F1 score (no labels): {:.1f} %'.format(100 * f1_score(true_lab, pred_lab[:, 0], average='macro')))\n",
    "print('Weighted F1 score: {:.1f} %'.format(100 * f1_score(true_lab, pred_lab[:, 0], labels=labels, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute top-K accuracies\n",
    "topk_accuracies = [test_utils.topK_accuracy(true_lab, pred_lab, K=k) for k in range(1, 6)]\n",
    "topk_accuracies = [acc * 100 for acc in topk_accuracies]  # Convert to percentages\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(range(1, 6), topk_accuracies, marker='o', linestyle='-', color='#007acc', linewidth=2)\n",
    "\n",
    "# Annotate each point\n",
    "for k, acc in enumerate(topk_accuracies, start=1):\n",
    "    ax.text(k, acc + 0.5, f\"{acc:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "# Customize axes and labels\n",
    "ax.set_xticks(range(1, 6))\n",
    "ax.set_xlabel('Top-K', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Top-K Accuracy (K=1 to 5)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(75, 100)\n",
    "ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Final layout and save\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute top-K accuracies (in %)\n",
    "topk_accuracies = np.array([test_utils.topK_accuracy(true_lab, pred_lab, K=k) for k in range(1, 6)]) * 100\n",
    "\n",
    "# Compute marginal increases\n",
    "marginal_increase = np.diff(topk_accuracies, prepend=topk_accuracies[0])\n",
    "marginal_increase[0] = np.nan  # No marginal increase for K=1\n",
    "\n",
    "# Plot\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = ax.bar(range(1, 6), marginal_increase, color='#ffa726', edgecolor='black')\n",
    "\n",
    "# Annotate bars\n",
    "for bar, inc in zip(bars, marginal_increase):\n",
    "    if not np.isnan(inc):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, height + 0.2, f\"{inc:.2f}%\", ha='center', fontsize=10)\n",
    "\n",
    "# Axis labels and formatting\n",
    "ax.set_xticks(range(1, 6))\n",
    "ax.set_xlabel('Top-K', fontsize=12)\n",
    "ax.set_ylabel('Marginal Increase in Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Marginal Accuracy Gain per K', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(marginal_increase[1:]) + 2)\n",
    "ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Save and show\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"topk_marginal_increase.png\", dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(data, save_path=None, weighted=False,comments=\"normal\"):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices and weighted confusion matrices for different thresholds using the provided data.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame containing the necessary data for plotting.\n",
    "        save_path (str): Path where the plots will be saved.\n",
    "        weighted (bool): Flag indicating whether to compute weighted confusion matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the labels\n",
    "    class_names = np.unique(data[\"top1\"])\n",
    "\n",
    "    # Define the range for iterations\n",
    "    myrange = np.arange(0, 1, 0.25)\n",
    "\n",
    "    # Create the directory for saving the plots\n",
    "    pred_path = create_pred_path(save_path, dir=\"confusion\", weighted=weighted, comments=comments)\n",
    "\n",
    "\n",
    "    # Iterate over the range\n",
    "    for i in myrange:\n",
    "        # Get the current time\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"[{current_time}] Plotting Confusion Matrix for {i}\")\n",
    "\n",
    "        # Filter the data based on a condition\n",
    "        slice = data[data[\"probability\"] > i]\n",
    "\n",
    "        # Extract true labels and predicted labels\n",
    "        y_true = np.array(slice[\"true_label\"])\n",
    "        y_pred = np.array(slice[\"top1\"])\n",
    "\n",
    "        y_true = y_true.astype(str)\n",
    "        y_pred = y_pred.astype(str)\n",
    "        # Compute the confusion matrix\n",
    "        weights = np.array(slice[\"probability\"]) if weighted else None\n",
    "        conf_mat = confusion_matrix(y_true, y_pred, labels=np.unique(y_pred), sample_weight=weights)\n",
    "\n",
    "        # Normalize the confusion matrix\n",
    "        normed_conf = conf_mat / np.sum(conf_mat, axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Plot the confusion matrix using the custom function plt_conf_matrix\n",
    "        fig, ax = plt_conf_matrix(normed_conf, class_names)\n",
    "\n",
    "        # Save the plot\n",
    "        # Determine the file name suffix based on weighted option\n",
    "        file_suffix = f\"weighted_{i:.2f}\" if weighted else f\"_{i:.2f}\"\n",
    "\n",
    "        # Save the confusion matrix plot\n",
    "        fig.savefig(f\"{pred_path}/confusion_matrix{file_suffix}.jpg\", bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    print(f\"Saved in {pred_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices(data, save_path=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted confusion matrix\n",
    "We weight each count in the confusion matrix by the probability of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory for saving the plots\n",
    "plot_confusion_matrices(data,save_path=False,weighted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics evolution in function of cut-off \n",
    "The evolution of differnt metrics is shown for different cut-off values. All data below this cut-off is discarted when plotting new metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolution_cut_off(data, save_path=False,comments=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the evolution of metrics with varying cutoff probability.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Input data containing \"probability\", \"top1\", and \"true_label\" columns.\n",
    "        save_path (str): Path where the plots will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None (plots are displayed and saved)\n",
    "    \"\"\"\n",
    "    myrange = np.arange(0, 1, 0.02)\n",
    "\n",
    "    pred_path = create_pred_path(save_path, dir=\"cut_off_changes\",comments=comments)\n",
    "\n",
    "    for cl in np.unique(data[\"top1\"]):\n",
    "        print(f\"Plotting cutoff probability for {cl}\")\n",
    "\n",
    "        metrics = []\n",
    "        data_slice = []\n",
    "\n",
    "        for i in myrange:\n",
    "            slice_data = data[data[\"probability\"] > i]\n",
    "            data_slice.append(compute_data_slice(slice_data, cl))\n",
    "\n",
    "            TP, FP, FN, TN = calculate_confusion_matrix(slice_data, cl)\n",
    "            precision, recall, outer_precision, outer_recall, f1_score, accuracy, specificity, fpr, fnr, auc_roc = compute_metrics(slice_data, cl, TP, FP, FN, TN)\n",
    "\n",
    "            metrics.append((precision, recall, outer_precision, outer_recall, f1_score, accuracy, specificity, fpr, fnr, auc_roc))\n",
    "\n",
    "        plot_metrics(myrange, metrics, data_slice, cl, pred_path)\n",
    "\n",
    "    print(\"Plots saved in:\", pred_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_data_slice(slice_data, cl):\n",
    "    return np.sum(slice_data['top1'] == cl) / np.sum(data['top1'] == cl)\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix(slice_data, cl):\n",
    "    TP = np.sum((slice_data[\"true_label\"] == cl) & (slice_data[\"top1\"] == cl))\n",
    "    FP = np.sum((slice_data[\"true_label\"] != cl) & (slice_data[\"top1\"] == cl))\n",
    "    FN = np.sum((slice_data[\"true_label\"] == cl) & (slice_data[\"top1\"] != cl))\n",
    "    TN = np.sum((slice_data[\"true_label\"] != cl) & (slice_data[\"top1\"] != cl))\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "\n",
    "def compute_metrics(slice_data, cl, TP, FP, FN, TN):\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    outer_precision = TP / np.sum(slice_data[\"top1\"] == cl)\n",
    "    outer_recall = TP / np.sum(slice_data[\"true_label\"] == cl)\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    fpr = FP / (FP + TN)\n",
    "    fnr = FN / (FN + TP)\n",
    "\n",
    "    if np.sum((slice_data[\"true_label\"] == cl)) != 0:\n",
    "        auc_roc = roc_auc_score((slice_data[\"true_label\"] == cl).astype(int), slice_data[\"probability\"])\n",
    "    else:\n",
    "        auc_roc = 0\n",
    "\n",
    "    return precision, recall, outer_precision, outer_recall, f1_score, accuracy, specificity, fpr, fnr, auc_roc\n",
    "\n",
    "\n",
    "def plot_metrics(myrange, metrics, data_slice, cl, pred_path):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    colors = cmap(np.linspace(0, 1, 12))\n",
    "\n",
    "    plt.plot(myrange, [m[0] for m in metrics], label=\"Precision\", linestyle='-', marker='o', color=colors[0])\n",
    "    plt.plot(myrange, [m[1] for m in metrics], label=\"Recall\", linestyle='--', marker='s', color=colors[1])\n",
    "    plt.plot(myrange, data_slice, label=\"Loss of Predictions\", linestyle='-', marker='*', color=colors[4])\n",
    "\n",
    "    plt.plot(myrange, [m[2] for m in metrics], label=\"Outer Precision\", linestyle=':', marker='^', color=colors[2])\n",
    "    plt.plot(myrange, [m[3] for m in metrics], label=\"Outer Recall\", linestyle='-.', marker='D', color=colors[3])\n",
    "\n",
    "    plt.plot(myrange, [m[4] for m in metrics], label=\"F1 Score\", linestyle='-', marker='x', color=colors[5])\n",
    "    plt.plot(myrange, [m[5] for m in metrics], label=\"Accuracy\", linestyle='--', marker='o', color=colors[6])\n",
    "    plt.plot(myrange, [m[6] for m in metrics], label=\"Specificity\", linestyle=':', marker='s', color=colors[7])\n",
    "    plt.plot(myrange, [m[7] for m in metrics], label=\"False Positive Rate\", linestyle='-.', marker='^', color=colors[8])\n",
    "    plt.plot(myrange, [m[8] for m in metrics], label=\"False Negative Rate\", linestyle='-', marker='D', color=colors[9])\n",
    "    plt.plot(myrange, [m[9] for m in metrics], label=\"AUC-ROC\", linestyle='--', marker='*', color=colors[10])\n",
    "\n",
    "    plt.legend(loc='best', fontsize='medium')\n",
    "    plt.xlabel(\"Cutoff value for probability\", fontsize='large')\n",
    "    plt.ylabel(\"Metric Score\", fontsize='large')\n",
    "    plt.title(cl, fontsize='x-large')\n",
    "\n",
    "    plt.xticks(fontsize='medium')\n",
    "    plt.yticks(fontsize='medium')\n",
    "\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.savefig(f\"{pred_path}/{cl.replace('/', '+')}.jpg\", bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution_cut_off(data, save_path=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Proportional and absolute progression of TRUE class is shown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classwise_progression(data,save_path,comments=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the class-wise progression of TRUE class distribution for different cutoff values.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame containing the necessary data for plotting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the range for iterations\n",
    "    myrange = np.arange(0, 1, 0.02)\n",
    "\n",
    "    # Create directory for saving results\n",
    "    pred_path = create_pred_path(save_path, dir=\"proportional_progression\",comments=comments)\n",
    "\n",
    "\n",
    "    for cl in np.unique(data[\"top1\"]):\n",
    "        print(f\"Plotting proportional progression for {cl}\")\n",
    "        compo = pd.DataFrame(data[data['top1'] == cl][\"true_label\"].value_counts())\n",
    "        for j, i in enumerate(myrange):\n",
    "            slice = data[data[\"probability\"] > i]\n",
    "            x = slice[slice['top1']==cl][\"true_label\"].value_counts()\n",
    "            compo[str(j)] = x\n",
    "\n",
    "\n",
    "        compo = compo.drop(\"count\", axis=1)\n",
    "        plotdata = compo / compo.sum()\n",
    "        plotdata1 = plotdata[plotdata > 0.01] * 100\n",
    "        plotdata2 = compo[plotdata > 0.01]\n",
    "        plotdata1.columns = myrange\n",
    "        plotdata2.columns = myrange\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        plotdata1.dropna(how=\"all\").transpose().plot.area(ax=axs[0], legend=False,\n",
    "                                                          title=\"proportional progression of TRUE class distribution\",\n",
    "                                                          xlabel=\"Cutoff value for probability\",\n",
    "                                                          ylabel=\"Class proportion (%)\")\n",
    "        plotdata2.dropna(how=\"all\").transpose().plot.area(ax=axs[1],\n",
    "                                                          title=\"absolute progression of TRUE class distribution\",\n",
    "                                                          xlabel=\"Cutoff value for probability\",\n",
    "                                                          ylabel=\"Class count\")\n",
    "        plt.suptitle(cl)\n",
    "\n",
    "        # Save the plot\n",
    "        plt.savefig(f\"{pred_path}/{cl.replace('/', '+')}.jpg\", bbox_inches='tight')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classwise_progression(data,save_path=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define cutoff - Nic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the cutoff probability for a given class\n",
    "def define_cutoff(data, cl, p_aim=0.95):\n",
    "    # Slice the data for the specific class and sort it in descending order based on probability\n",
    "    slice = data[data[\"top1\"] == cl].sort_values(by=\"probability\", ascending=False).reset_index()\n",
    "    # Calculate the precision for each position in the slice\n",
    "    prec_list = np.cumsum(slice[\"top1_correct\"]) / (slice.index + 1)\n",
    "    # Find the positions where precision exceeds the desired aim\n",
    "    acceptable_probabs = np.where(prec_list > p_aim)[0]\n",
    "    if len(acceptable_probabs) > 0:\n",
    "        # Return the minimum probability from the acceptable positions\n",
    "        return np.min(slice['probability'][acceptable_probabs])\n",
    "    else:\n",
    "        # If no acceptable positions, return infinity\n",
    "        return np.inf\n",
    "\n",
    "# Function to apply the mask to the data based on the cutoff probabilities\n",
    "def apply_mask(data, mask_data):\n",
    "    # Initialize a mask with False values for each data point\n",
    "    data_mask = np.repeat(False, len(data))\n",
    "    for _, x in mask_data.iterrows():\n",
    "        # Update the mask where the class and probability satisfy the cutoff criteria\n",
    "        data_mask[(data[\"top1\"] == x[\"Class\"]) & (data[\"probability\"] >= x[\"probability_cutoff\"])] = True\n",
    "    # Return the masked data\n",
    "    return data[data_mask]\n",
    "\n",
    "def mask_creater(p_aim,data):\n",
    "    mask = [[x, define_cutoff(data, x, p_aim=p_aim)] for x in np.unique(data['top1'])]\n",
    "    mask_data = pd.DataFrame(mask, columns=['Class', \"probability_cutoff\"])\n",
    "    masked_data = apply_mask(data, mask_data)\n",
    "    return masked_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary_threshold(data, save_path=False,comments=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the scores on a masked DataFrame with aimed precision.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame containing the necessary data for plotting.\n",
    "        name (str): Name of the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the range of precision values to aim for\n",
    "    myrange = np.concatenate((np.arange(0.5, 0.95, 0.01), np.arange(0.95, 0.995, 0.005)))\n",
    "\n",
    "    # Create directory for saving results\n",
    "    pred_path = create_pred_path(save_path, dir=\"summary_threshold\",comments=comments)\n",
    "\n",
    "    # Initialize arrays to store the evaluation metrics\n",
    "    acc= np.zeros(len(myrange))\n",
    "    pr_weighted = np.zeros(len(myrange))\n",
    "    rec_weighted = np.zeros(len(myrange))\n",
    "    f1_weighted = np.zeros(len(myrange))\n",
    "    pr_macro = np.zeros(len(myrange))\n",
    "    rec_macro = np.zeros(len(myrange))\n",
    "    f1_macro = np.zeros(len(myrange))\n",
    "    dataset_prop = np.zeros(len(myrange))\n",
    "    class_prop = np.zeros(len(myrange))\n",
    "\n",
    "    # Calculate evaluation metrics for each precision value\n",
    "    for i, p_aim in enumerate(tqdm(myrange, desc='Processing')):\n",
    "        masked_data =mask_creater(p_aim,data)\n",
    "\n",
    "        dataset_prop[i] = len(masked_data) / len(data)\n",
    "        class_prop[i] = len(np.unique(masked_data[\"top1\"])) / len(np.unique(data[\"top1\"]))\n",
    "        acc[i] = accuracy_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"])#, average=\"weighted\")\n",
    "        pr_weighted[i] = precision_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"weighted\")\n",
    "        rec_weighted[i] = recall_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"weighted\")\n",
    "        f1_weighted[i] = f1_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"weighted\")\n",
    "        # pr_macro[i] = precision_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"macro\", zero_division=1)\n",
    "        # rec_macro[i] = recall_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"macro\", zero_division=1)\n",
    "        # f1_macro[i] = f1_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"macro\", zero_division=1)\n",
    "        # acc[i] = accuracy_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"])\n",
    "        # pr_micro[i] = precision_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"micro\",)\n",
    "        # rec_micro[i] = recall_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"micro\")\n",
    "        # f1_micro[i] = f1_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"micro\")\n",
    "        # pr_macro[i] = precision_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"macro\", zero_division=1)\n",
    "        # rec_macro[i] = recall_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"macro\", zero_division=1)\n",
    "        # f1_macro[i] = f1_score(y_true=masked_data[\"true_label\"], y_pred=masked_data[\"top1\"], average=\"macro\", zero_division=1)\n",
    "\n",
    "    print(\"done!\")\n",
    "\n",
    "    # Create a DataFrame to show the evaluation metrics for different precision values\n",
    "    show_data = pd.DataFrame({\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision (weighted)\": pr_weighted,\n",
    "        \"Recall (weighted)\": rec_weighted,\n",
    "        \"F1 Score (weighted)\": f1_weighted,\n",
    "        \"Dataset Proportion\": dataset_prop,\n",
    "        \"Class Proportion\": class_prop\n",
    "    }, index=myrange)\n",
    "\n",
    "    # Plot the evaluation metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Define custom colors for each metric line\n",
    "    colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\"]#, \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"]\n",
    "\n",
    "    # Plot each metric with a different color and line style\n",
    "    for i, metric in enumerate(show_data.columns):\n",
    "        if metric in [\"Dataset Proportion\", \"Class Proportion\"]:\n",
    "            plt.plot(show_data.index, show_data[metric], marker='o', linestyle='--', color=colors[i], label=metric, alpha=0.7)\n",
    "        elif metric==\"Accuracy\":\n",
    "            # print(show_data[metric])\n",
    "            plt.plot(show_data.index, show_data[metric], marker='x', color=colors[i], label=metric, alpha=1,markersize=10)\n",
    "        else:\n",
    "            plt.plot(show_data.index, show_data[metric], marker='o', color=colors[i], label=metric, alpha=0.7)\n",
    "\n",
    "    plt.title(\"Scores on after masking increasing thresholds\")\n",
    "    plt.xlabel(\"Probability Threshold \")\n",
    "    plt.xticks(rotation=45)\n",
    "    # plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    filename = os.path.join(pred_path, \"summary_threshold_plot.jpg\")\n",
    "    plt.savefig(filename)\n",
    "    print(\"Saved in\", filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summary_threshold(data,comments=\"saved_0.5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
